# Linux系统IO原理

## 书籍推荐
《深入理解Linux内核》《深入理解计算机系统》

## 面试问题
epoll是怎么知道数据到达、可以通知用户读取的？  
这里面有一个中断的概念，调用了epoll_wait
补充pagecache的知识：  
内存分配的时候是大家在物理内存中共同使用一个kernel，然后是各个应用程序都打散，分布到内存的各个地方，因为他们并不是一口气被分配出来的，而是
用到哪一部分就分配哪一部分，程序以碎片的方式在物理空间里，但是虚拟地址中他自己是独占的。这里面就牵扯到一个问题：虚拟内存中线性地址连续的两段
地址，可能被映射到不同的物理地址，这需要一个MMU，程序里还要把映射表提供给MMU去换算这个映射的关系表。硬件的MMU所依赖的是page，一页在物理内存
中的大小是默认4kB，管理物理内存线性地址的时候，一堆4k的小格子，程序用谁了，找个空的4k小格子，把数据放进去，并映射上。但是，系统不会全量分配，
所以有时候会有缺页异常，类似于软中断，跑到内核里面，开始建立页的映射关系，把页补上，然后在漂移回来，染CPU继续执行指令。C的操作系统进程跑起来之后，
有这么几段：代码段、数据段、栈、堆，寄存器会指向这几个段的基地址，然后给出便宜就知道具体哪个位置了。先从硬盘把程序文件appW(binary)加载到内存，
这个时候也会触发page，读4k先放进来。假设文件有10个4k，但是可能先房间一个4k，随着访问的需要，再继续分配。以4k为单位，按需分配。  

IO的依赖是pagecache，虽然C的direct IO能够跳过内核的pagecache，但是Java里面没有direct IO，两java的mmap也是用的pagecache
```
[root@localhost ~]# sysctl -a | grep dirty
vm.dirty_background_bytes = 0
vm.dirty_background_ratio = 10
vm.dirty_bytes = 0
vm.dirty_expire_centisecs = 3000
vm.dirty_ratio = 30
vm.dirty_writeback_centisecs = 500
```
`vim /etc/sysctl.conf`修改以上的参数：
`vm.dirty_background_ratio = 90` 改为90%。假设还有10G可用，缓存占到9G的时候，才会有内核完成由内存到磁盘的写的过程，3G的内存，我们向
内存写2个多G之后才会真正落地到磁盘，这个可能丢很多数据。这是后台线程  
`vm.dirty_ratio = 90` 前台线程。程序往内核写的时候分配页已经分配到90%了（比前面的后台ratio要设置的稍微高一点），假设程序疯狂的向内核写
数据，再疯狂的分配pagecache，以达到可用内存的90%，后台的写操作已经挡不值这个比例增长到90%了，这时候就阻塞程序（上面的不会阻塞，程序继续写，
会起一个后台线程向磁盘写，如果线程比程序写
的快，则内存不会爆满），这时候把脏页的数据写到磁盘上，再写的话两个都会触发LRU，把老的、没怎么用的页淘汰出去，保证新数据能写到内存里。说白了就是
程序先写内存后写硬盘，只不过是什么时候写的问题。这两个设置可以关联到Redis做持久化aof、mySQL调有的时候的bin log等，它们都是有三个级别可以调，
一个是每秒钟写一次、随内核、每操作写一次，就是因为不是立即保存，有可能丢很多。现在修改后台线程时间的维度，脏页能存多久（单位是百分之一秒），
多长时间就要做一次写回硬盘：
```
vm.dirty_expire_centisecs = 30000
vm.dirty_writeback_centisecs = 50000
```
目的是不让时间成为干扰. 改完了之后要执行`sysctl -p`执行更新生效就想看一直写的话，突然关机，能丢多少数据。写的时候分配的页，一开始申请过来
就是脏的，往内存里写过了就不是脏的了。不脏的页，可以背LRU或者LFU淘汰掉，如果是脏的页，那不能在内存中抹杀掉，是要先写到磁盘中才能淘汰掉。
现在(192.168.199上)执行：`/root/mysh 0` 试图往磁盘里写数据：
```
[root@localhost testfileio]# ll -h && pcstat out.txt
total 79M
-rwxr-xr-x. 1 root root  112 Nov  8 17:13 mysh
-rw-r--r--. 1 root root 4.0K Nov  8 17:39 OSFileIO.class
-rwxr-xr-x. 1 root root 4.5K Nov  8 17:39 OSFileIO.java
-rw-r--r--. 1 root root 9.5K Nov  8 17:39 out.71970
-rw-r--r--. 1 root root  52M Nov  8 17:40 out.71971
-rw-r--r--. 1 root root 4.0K Nov  8 17:40 out.71972
-rw-r--r--. 1 root root  928 Nov  8 17:39 out.71973
-rw-r--r--. 1 root root 1.1K Nov  8 17:39 out.71974
-rw-r--r--. 1 root root  972 Nov  8 17:39 out.71975
-rw-r--r--. 1 root root 6.7K Nov  8 17:40 out.71976
-rw-r--r--. 1 root root 4.6K Nov  8 17:40 out.71977
-rw-r--r--. 1 root root  928 Nov  8 17:39 out.71978
-rw-r--r--. 1 root root  45K Nov  8 17:40 out.71979
-rw-r--r--. 1 root root  12M Nov  8 17:40 out.txt
+---------+----------------+------------+-----------+---------+
| Name    | Size (bytes)   | Pages      | Cached    | Percent |
|---------+----------------+------------+-----------+---------|
| out.txt | 11951370       | 2918       | 2918      | 100.000 |
+---------+----------------+------------+-----------+---------+
```
只要内存够，pagecache就一直缓存。未来想读头的时候，不需要产生磁盘IO就可以读到。可以看到，似乎写了不上，虽然速度不是太快（sleep已经注释掉了，
减少了系统调用，没有用内切换）。但是现在直接"拔电源"，然后再次启动回来再查看pagecache的情况：
```
Shuzheng-Mac2:Documents shuzheng$ ssh root@192.168.1.99
Last login: Sun Nov  8 17:56:39 2020 from 192.168.1.102
[root@localhost ~]# cd testfileio/
[root@localhost testfileio]# ll -h
total 16K
-rwxr-xr-x. 1 root root  112 Nov  8 17:13 mysh
-rw-r--r--. 1 root root 4.0K Nov  8 18:08 OSFileIO.class
-rwxr-xr-x. 1 root root 4.5K Nov  8 17:39 OSFileIO.java
-rw-r--r--. 1 root root    0 Nov  8 18:08 out.2494
-rw-r--r--. 1 root root    0 Nov  8 18:09 out.2495
-rw-r--r--. 1 root root    0 Nov  8 18:09 out.2496
-rw-r--r--. 1 root root    0 Nov  8 18:08 out.2497
-rw-r--r--. 1 root root    0 Nov  8 18:08 out.2498
-rw-r--r--. 1 root root    0 Nov  8 18:08 out.2499
-rw-r--r--. 1 root root    0 Nov  8 18:09 out.2500
-rw-r--r--. 1 root root    0 Nov  8 18:09 out.2501
-rw-r--r--. 1 root root    0 Nov  8 18:08 out.2502
-rw-r--r--. 1 root root    0 Nov  8 18:09 out.2503
-rw-r--r--. 1 root root    0 Nov  8 18:09 out.txt
```
可见由于内核参数配置的太高了，OS并没有来得及保存到硬盘。这就是为什么Redis不是完全信任内核，而是设为1s。
现在测试用`BufferedOutputStream` 来往磁盘里写，也就是在FileOutputStream外面套一层BufferedOutputStream，还是要注释掉sleep，不带
flush()，BufferedOutputStream为什么快？因为JVM有一个默认的8kB的buffer，一旦写满了这8kB才会有syscall写入硬盘，否则每次只有10个字节就有
syscall的话就太慢了。
执行：`/root/mysh 0`  写数据可见一开始cached栈100%，但是越往后越小, pages越写越多，但是能cache住的就那么多，所以写同一个文件时，内存
到达上限就会写新的+淘汰老的，cache大体不变，但是cache住的pages的比例会越来越小。
```
[root@localhost testfileio]# ll -h && pcstat out.txt
total 923M
-rwxr-xr-x. 1 root root  112 Nov  8 17:13 mysh
-rw-r--r--. 1 root root 3.9K Nov  8 23:33 OSFileIO.class
-rwxr-xr-x. 1 root root 4.5K Nov  8 23:25 OSFileIO.java
-rw-r--r--. 1 root root 9.5K Nov  8 23:33 out.5277
-rw-r--r--. 1 root root 4.8M Nov  8 23:33 out.5278
-rw-r--r--. 1 root root 2.6K Nov  8 23:33 out.5279
-rw-r--r--. 1 root root  925 Nov  8 23:33 out.5280
-rw-r--r--. 1 root root 1.1K Nov  8 23:33 out.5281
-rw-r--r--. 1 root root  969 Nov  8 23:33 out.5282
-rw-r--r--. 1 root root 8.4K Nov  8 23:33 out.5283
-rw-r--r--. 1 root root 5.7K Nov  8 23:33 out.5284
-rw-r--r--. 1 root root  925 Nov  8 23:33 out.5285
-rw-r--r--. 1 root root  18K Nov  8 23:33 out.5286
-rw-r--r--. 1 root root 589M Nov  8 23:33 out.txt
+---------+----------------+------------+-----------+---------+
| Name    | Size (bytes)   | Pages      | Cached    | Percent |
|---------+----------------+------------+-----------+---------|
| out.txt | 621375300      | 151703     | 151703    | 100.000 |
+---------+----------------+------------+-----------+---------+
...
...
[root@localhost testfileio]# ll -h && pcstat out.txt
total 11G
-rwxr-xr-x. 1 root root  112 Nov  8 17:13 mysh
-rw-r--r--. 1 root root 3.9K Nov  8 23:33 OSFileIO.class
-rwxr-xr-x. 1 root root 4.5K Nov  8 23:25 OSFileIO.java
-rw-r--r--. 1 root root 9.6K Nov  8 23:34 out.5277
-rw-r--r--. 1 root root  83M Nov  8 23:34 out.5278
-rw-r--r--. 1 root root  14K Nov  8 23:34 out.5279
-rw-r--r--. 1 root root  967 Nov  8 23:34 out.5280
-rw-r--r--. 1 root root 1.1K Nov  8 23:34 out.5281
-rw-r--r--. 1 root root 2.1K Nov  8 23:34 out.5282
-rw-r--r--. 1 root root  11K Nov  8 23:34 out.5283
-rw-r--r--. 1 root root 7.9K Nov  8 23:34 out.5284
-rw-r--r--. 1 root root  967 Nov  8 23:34 out.5285
-rw-r--r--. 1 root root 210K Nov  8 23:34 out.5286
-rw-r--r--. 1 root root 1.9K Nov  8 23:34 out.5544
-rw-r--r--. 1 root root  11G Nov  8 23:34 out.txt
+---------+----------------+------------+-----------+---------+
| Name    | Size (bytes)   | Pages      | Cached    | Percent |
|---------+----------------+------------+-----------+---------|
| out.txt | 11012732640    | 2688656    | 207089    | 007.702 |
+---------+----------------+------------+-----------+---------+
```
重命名但是配置并不丢失：
```
[root@localhost testfileio]# mv out.txt ooxx.txt
[root@localhost testfileio]# pcstat ooxx.txt 
+----------+----------------+------------+-----------+---------+
| Name     | Size (bytes)   | Pages      | Cached    | Percent |
|----------+----------------+------------+-----------+---------|
| ooxx.txt | 11012732640    | 2688656    | 207089    | 007.702 |
+----------+----------------+------------+-----------+---------+
```
重命名之后再次新建并写out.txt：
```
[root@localhost testfileio]# ll -h && pcstat out.txt && pcstat ooxx.txt 
total 13G
-rwxr-xr-x. 1 root root  112 Nov  8 17:13 mysh
-rw-r--r--. 1 root root  11G Nov  8 23:34 ooxx.txt
-rw-r--r--. 1 root root 3.9K Nov  8 23:54 OSFileIO.class
-rwxr-xr-x. 1 root root 4.5K Nov  8 23:25 OSFileIO.java
-rw-r--r--. 1 root root 9.6K Nov  8 23:54 out.5773
-rw-r--r--. 1 root root  21M Nov  8 23:54 out.5774
-rw-r--r--. 1 root root 4.6K Nov  8 23:54 out.5775
-rw-r--r--. 1 root root  965 Nov  8 23:54 out.5776
-rw-r--r--. 1 root root 1.1K Nov  8 23:54 out.5777
-rw-r--r--. 1 root root 1.3K Nov  8 23:54 out.5778
-rw-r--r--. 1 root root 8.7K Nov  8 23:54 out.5779
-rw-r--r--. 1 root root 6.2K Nov  8 23:54 out.5780
-rw-r--r--. 1 root root  965 Nov  8 23:54 out.5781
-rw-r--r--. 1 root root  55K Nov  8 23:54 out.5782
-rw-r--r--. 1 root root 2.6G Nov  8 23:54 out.txt
+---------+----------------+------------+-----------+---------+
| Name    | Size (bytes)   | Pages      | Cached    | Percent |
|---------+----------------+------------+-----------+---------|
| out.txt | 2741399552     | 669287     | 208733    | 031.187 |
+---------+----------------+------------+-----------+---------+
+----------+----------------+------------+-----------+---------+
| Name     | Size (bytes)   | Pages      | Cached    | Percent |
|----------+----------------+------------+-----------+---------|
| ooxx.txt | 11012732640    | 2688656    | 0         | 000.000 |
+----------+----------------+------------+-----------+---------+
```
可见很快历史的文件ooxx.txt缓存的cache就降低为0了，都被淘汰了，因为有另一个在写, 但是淘汰掉的脏页也被刷写到磁盘上去了。  

pagecache的好处是又花了IO性能，缺点是可能会丢失数据. 用户调用flush()就会强制把脏页写到磁盘。一个脏页pagecache是4k大小。如果用户调用write
的时候刚1k，内核准备了一个4k的格子（buffer），刚放了1k，一调用flush(), 整个4k都要写到磁盘上；第二次写进来1k到pagecache的时候，是占用从1-2k，
然后这个页又变成脏的了，再次flush(), 又会把这2k的内容和2k空白，一共4k，写到磁盘上去。这个过程是用内核管理的。缓存可以在进程、设备、内核三个
地方中，这三个地方都有可能丢数据。LRU】淘汰的pagecache，必须检查一下是不是dirty，如果是dirty，则要先刷写到磁盘才能淘汰掉。cache的阈值设成
90%并且达到的时候，虽然还有10%的空间，但是这时候就把最不常用的pagecache往磁盘里面写了，然后把pagecache标注为不是dirty，而pagecache还在那里，
cache总是尽量为我们保存着。刚创建或修改完的pagecache都是脏的。脏了被同步到磁盘不到表他会被删掉，只不过状态会变化，内存不够了这种cache的才会
被淘汰掉。dirty被写入磁盘，可以是用户的flush()，可以是内核达到阈值了，有可能是别的程序往里面写触发了LRU要被他淘汰掉，在被淘汰掉之前如果是脏的，
就往磁盘里写入之后再淘汰掉. 内核里面会维护每一个pagecache的索引，类似于一个Object，里面有一个dirty属性（dirty状态位，除此之外还有inode）。
某一个pagecache被淘汰了之后，在想访问，又会触发缺页异常。在往磁盘写入pagecache内容的时候，可以触发DMA，把总线交给他，减轻CPU负担。DMA是很
古老的，一直被使用的东西，在块设备时候必然会使用的一个功能，没有DMA协处理器的话也可以。CPU在没有DMA的情况下，内存里的数据会经过寄存器，从CPU
的数据总线交给硬盘，会从CPU这里转圈转出去，CPU这时候是不能位其他的进程去服务的，因为他一直在忙IO这件事。DMA这个技术不怎么值钱，刚需必配的。  

pagecache是不是JVM占用的内存之外的内存？  
不是的。JVM是虚拟词汇，java程序的代码段可以看成是虚拟机的逻辑。是一段code，用C写的，是一个Java进程，他会在堆里分配内存空间，如果他分配了一个G的空间，
这就是JVM的堆大小。Java进程的堆在OS里面可以使用很大个儿，只不过他只开辟了1G的空间，只给JVM的寻址范围，这就是JVM的-Xmx的大小。当Java程序被load成为一个进程的时候，这里会用
pagecache分页，这个1G的堆也是若干个4k，只不过这些4k不是跟磁盘IO对应关系，只是在内存里申请了一些运行时的页，以为在内核管理内存的时候，他用"页"
做了整个的一个抽象，只要是程序的虚拟的线性地址，某一个地址区间一定是随着页分配的，内核空间不是堆空间，一切都可以想象成是page组成的。真正的
pagecache是一个地址区间的映射，从虚拟逻辑地址的某一块地址，通过MMU，映射到物理地址的一块地址，有一个页表，分为1-4级。  

## 追踪
跑一下下不用buffer直接写硬盘的程序：
```
[root@localhost testfileio]# ./mysh 0
^C[root@localhost testfileio]# ll
total 1864
-rwxr-xr-x. 1 root root     112 Nov  8 17:13 mysh
-rw-r--r--. 1 root root    3989 Nov 14 00:01 OSFileIO.class
-rwxr-xr-x. 1 root root    4545 Nov  8 23:25 OSFileIO.java
-rw-r--r--. 1 root root    9806 Nov 14 00:01 out.2335
-rw-r--r--. 1 root root 1528823 Nov 14 00:01 out.2336
-rw-r--r--. 1 root root    1415 Nov 14 00:01 out.2337
-rw-r--r--. 1 root root     967 Nov 14 00:01 out.2338
-rw-r--r--. 1 root root    1091 Nov 14 00:01 out.2339
-rw-r--r--. 1 root root    2249 Nov 14 00:01 out.2340
-rw-r--r--. 1 root root    6028 Nov 14 00:01 out.2341
-rw-r--r--. 1 root root    3409 Nov 14 00:01 out.2342
-rw-r--r--. 1 root root     967 Nov 14 00:01 out.2343
-rw-r--r--. 1 root root    2461 Nov 14 00:01 out.2344
-rw-r--r--. 1 root root    1903 Nov 14 00:01 out.2345
-rw-r--r--. 1 root root  304300 Nov 14 00:01 out.txt
```
out文件后面跟着的后缀是线程号，JVM其实是个多线程，整个里面不只是我们的main在运行。查看最大的那一个，可以看到strace追踪的，应用程序对内核的
系统调用，这里发现了一堆write：
```
fstat(4, {st_mode=S_IFREG|0644, st_size=0, ...}) = 0
write(4, "123456789\n", 10)             = 10
write(4, "123456789\n", 10)             = 10
write(4, "123456789\n", 10)             = 10
write(4, "123456789\n", 10)             = 10
write(4, "123456789\n", 10)             = 10
write(4, "123456789\n", 10)             = 10
write(4, "123456789\n", 10)             = 10
write(4, "123456789\n", 10)             = 10
write(4, "123456789\n", 10)             = 10
write(4, "123456789\n", 10)             = 10
write(4, "123456789\n", 10)             = 10
write(4, "123456789\n", 10)             = 10
... ...
... ...
```
out.txt的大小是：304300
每一行其实都是一个系统调用，也就是用户态到内核态的转换，这一过程会有寄存器保存现场等一系列复杂的、消耗CPU指令时间的事情。所以像这样直接写
而不使用buffer的时候，out.txt提及增长比较慢，就是因为单位时间内，做了太多的用内切换。
这次执行需要buffer的那一个：
```
[root@localhost testfileio]# ./mysh 1
^C[root@localhost testfileio]# ll
total 479412
-rwxr-xr-x. 1 root root       112 Nov  8 17:13 mysh
-rw-r--r--. 1 root root      3989 Nov 14 00:11 OSFileIO.class
-rwxr-xr-x. 1 root root      4545 Nov  8 23:25 OSFileIO.java
-rw-r--r--. 1 root root     10000 Nov 14 00:11 out.2459
-rw-r--r--. 1 root root   3965153 Nov 14 00:11 out.2460
-rw-r--r--. 1 root root      2820 Nov 14 00:11 out.2461
-rw-r--r--. 1 root root       967 Nov 14 00:11 out.2462
-rw-r--r--. 1 root root      1091 Nov 14 00:11 out.2463
-rw-r--r--. 1 root root      2096 Nov 14 00:11 out.2464
-rw-r--r--. 1 root root      8028 Nov 14 00:11 out.2465
-rw-r--r--. 1 root root      5636 Nov 14 00:11 out.2466
-rw-r--r--. 1 root root       967 Nov 14 00:11 out.2467
-rw-r--r--. 1 root root     14986 Nov 14 00:11 out.2468
-rw-r--r--. 1 root root      1787 Nov 14 00:11 out.2470
-rw-r--r--. 1 root root 486862740 Nov 14 00:11 out.txt
```
打开 out.2460文件
```
write(4, "123456789\n123456789\n123456789\n12"..., 8190) = 8190
write(4, "123456789\n123456789\n123456789\n12"..., 8190) = 8190
write(4, "123456789\n123456789\n123456789\n12"..., 8190) = 8190
write(4, "123456789\n123456789\n123456789\n12"..., 8190) = 8190
write(4, "123456789\n123456789\n123456789\n12"..., 8190) = 8190
write(4, "123456789\n123456789\n123456789\n12"..., 8190) = 8190
```
两次执行差不多的时间，out.txt的大小是：486862740，而且发现，这次write的可就不止一个"123456789\n"了，因为BufferedOutputStream的方法
有一个8k的缓冲区在JVM内存里。他应用了缓冲区解决系统调用的这种损耗，网络IO的多路复用也是大大减少了系统调用

## nio的ByteBuffer和FileChannel
FileChannel把输入输出都集成了，也是基于Buffer的，并没有超越前面的形式，只不过API是新的，开扩展了新的MappedByteBuffer、DirectBuffer
ByteBuffer buffer = ByteBuffer.allocateDirect(1024);和ByteBuffer buffer = ByteBuffer.allocate(1024);区别在是不是分配在堆上，
后者是分配在堆上。就想像成他是个字节数组，有4个维度：偏移指针pos，大小限制limit，总大小capacity，以上三个维度mark。pos指向0，limit和cap
指向1024。flip反转的意思是在读取的时候，pos回到开始的位置，又怕读超过了，所以limit要回到pos刚才的位置，flip就是：limit = pos; pos = 0;
get在读出一个字节的时候也会向后移动一个pos。compact：把前面读的一个依次用后面的字符往前覆盖掉，pos变成2，下一个要读的位置，limit再指向1024，
pos前面是曾经往里写还没有get完的，后面是剩余可用的。想写的时候compact想读的时候flip一下clear是都从新归位。  

只有硬盘中的文件可以有map（内存映射）方法，因为它是块设备，可以来回自由的寻址去读取。只有文件才可以做内存映射，把内核的pagecache和文件的
数据页的地址空间映射起来。只有FileChannel.map可以得到一个 MappedByteBuffer. 这里用了mmap  堆外  和文件映射的ByteBuffer   byte  not  objtect。
一般来说，Java先从JVM的堆里面拷贝到Java进程的C的堆里面，然后再拷贝到内核的空间里，然后再写入设备里。堆外内存这个事儿，是大数据和后端高级职称都要去考虑的。
MappedByteBuffer.put不是系统调用，表面上看是往ByteBuffer这个字节数组里放了内容，但是由于这个特殊的ByteBuffer和文件映射了，其实就映射到了
内核的pagecache，数据会到达内核的pagecache。曾经我们是需要out.write()  这样的系统调用，才能让程序的data 进入内核的pagecache，曾经必须有
用户态内核态切换。mmap的内存映射，依然是内核的pagecache体系所约束的！！！换言之，丢数据。你可以去github上找一些 其他C程序员写的jni扩展库，
使用linux内核的Direct IO。直接IO是忽略linux的pagecache，是把pagecache  交给了程序自己开辟一个字节数组当作pagecache，动用代码逻辑来维护一致性/dirty。。。
一系列复杂问题. 坏处是pagecache不共享了，某个程序自己的，自己管理pagecache了。该丢丢、该慢慢只不过可以控制的力度更细一些，因为内核的话一旦调整配置项，
他都是对全局生效的。数据库一般会使用Direct IO。

## 验证
运行`mysh 2`, 并在另一个窗口下执行`lsof -p 8262`，然后另一个窗口中查看：
```
[root@localhost testfileio]# ll
total 1272
-rwxr-xr-x. 1 root root    112 Nov  8 17:13 mysh
-rw-r--r--. 1 root root   3989 Nov 14 23:48 OSFileIO.class
-rwxr-xr-x. 1 root root   4545 Nov  8 23:25 OSFileIO.java
-rw-r--r--. 1 root root   9663 Nov 14 23:48 out.8262
-rw-r--r--. 1 root root 159908 Nov 14 23:48 out.8263
-rw-r--r--. 1 root root  41737 Nov 14 23:52 out.8264
-rw-r--r--. 1 root root    925 Nov 14 23:48 out.8265
-rw-r--r--. 1 root root   1049 Nov 14 23:48 out.8266
-rw-r--r--. 1 root root    969 Nov 14 23:48 out.8267
-rw-r--r--. 1 root root  13158 Nov 14 23:52 out.8268
-rw-r--r--. 1 root root  10786 Nov 14 23:52 out.8269
-rw-r--r--. 1 root root    925 Nov 14 23:48 out.8270
-rw-r--r--. 1 root root 800592 Nov 14 23:52 out.8271
-rw-r--r--. 1 root root     31 Nov 14 23:48 out.txt
```
这里有个`/root/testfileio/out.txt`他其实并不在磁盘上，虽然`cat /root/testfileio/out.txt`会显示：
```
[root@localhost testfileio]# cat out.txt
hello mashibing
hello seanzhou
```
其实文件内容是写在了pagecache中。 在原窗口中再回车，可以看到，seek------，此时在第四个字节处写了一个ooxx这会把原来的覆盖掉。再次执行cat
查看：
```
[root@localhost testfileio]# cat out.txt
hellooxxshibing
hello seanzhou
```
这就是随机读写。  

验证mmap内存映射：
```
[root@localhost ~]# lsof -p 8262
COMMAND  PID USER   FD   TYPE DEVICE  SIZE/OFF     NODE NAME
java    8262 root  cwd    DIR    8,3       236  2093932 /root/testfileio
java    8262 root  rtd    DIR    8,3       224       64 /
java    8262 root  txt    REG    8,3      7734 53030349 /usr/java/jdk1.8.0_181-amd64/bin/java
java    8262 root  mem    REG    8,3 106070960 17091920 /usr/lib/locale/locale-archive
java    8262 root  mem    REG    8,3  66044248   210251 /usr/java/jdk1.8.0_181-amd64/jre/lib/rt.jar
java    8262 root  mem    REG    8,3    128794 18643342 /usr/java/jdk1.8.0_181-amd64/jre/lib/amd64/libzip.so
java    8262 root  mem    REG    8,3     61752   117379 /usr/lib64/libnss_files-2.17.so
java    8262 root  mem    REG    8,3    226512 18834006 /usr/java/jdk1.8.0_181-amd64/jre/lib/amd64/libjava.so
java    8262 root  mem    REG    8,3     66472 18643341 /usr/java/jdk1.8.0_181-amd64/jre/lib/amd64/libverify.so
java    8262 root  mem    REG    8,3     43928   117391 /usr/lib64/librt-2.17.so
java    8262 root  mem    REG    8,3   1141456    93305 /usr/lib64/libm-2.17.so
java    8262 root  mem    REG    8,3  17068604 50673372 /usr/java/jdk1.8.0_181-amd64/jre/lib/amd64/server/libjvm.so
java    8262 root  mem    REG    8,3   2116736    93297 /usr/lib64/libc-2.17.so
java    8262 root  mem    REG    8,3     19344    93303 /usr/lib64/libdl-2.17.so
java    8262 root  mem    REG    8,3    104289 17064288 /usr/java/jdk1.8.0_181-amd64/lib/amd64/jli/libjli.so
java    8262 root  mem    REG    8,3    143352   117387 /usr/lib64/libpthread-2.17.so
java    8262 root  mem    REG    8,3    155064    93290 /usr/lib64/ld-2.17.so
java    8262 root  mem    REG    8,3     32768 51320817 /tmp/hsperfdata_root/8262
java    8262 root    0u   CHR  136,0       0t0        3 /dev/pts/0
java    8262 root    1u   CHR  136,0       0t0        3 /dev/pts/0
java    8262 root    2u   CHR  136,0       0t0        3 /dev/pts/0
java    8262 root    3r   REG    8,3  66044248   210251 /usr/java/jdk1.8.0_181-amd64/jre/lib/rt.jar
java    8262 root    4u   REG    8,3        31  1951892 /root/testfileio/out.txt
```
可以看到，/root/testfileio/out.txt 并没有在mem里面，最后一行是开启的一个普通读写的文件描述符。mem是所有内存分配引起的，目前是只有各个jar
包里面做的内存分配有mem。第三行是java的代码段（txt），可执行程序。再次在原窗口中按下回车，并再次在另一窗口执行`lsof -p 8262`可得:
```
[root@localhost testfileio]# lsof -p 8262
COMMAND  PID USER   FD   TYPE             DEVICE  SIZE/OFF     NODE NAME
java    8262 root  cwd    DIR                8,3       236  2093932 /root/testfileio
java    8262 root  rtd    DIR                8,3       224       64 /
java    8262 root  txt    REG                8,3      7734 53030349 /usr/java/jdk1.8.0_181-amd64/bin/java
java    8262 root  mem    REG                8,3 106070960 17091920 /usr/lib/locale/locale-archive
java    8262 root  mem    REG                8,3     93308 17037054 /usr/java/jdk1.8.0_181-amd64/jre/lib/amd64/libnio.so
java    8262 root  mem    REG                8,3    115485 17034961 /usr/java/jdk1.8.0_181-amd64/jre/lib/amd64/libnet.so
java    8262 root  mem    REG                8,3  66044248   210251 /usr/java/jdk1.8.0_181-amd64/jre/lib/rt.jar
java    8262 root  mem    REG                8,3    128794 18643342 /usr/java/jdk1.8.0_181-amd64/jre/lib/amd64/libzip.so
java    8262 root  mem    REG                8,3     61752   117379 /usr/lib64/libnss_files-2.17.so
java    8262 root  mem    REG                8,3    226512 18834006 /usr/java/jdk1.8.0_181-amd64/jre/lib/amd64/libjava.so
java    8262 root  mem    REG                8,3     66472 18643341 /usr/java/jdk1.8.0_181-amd64/jre/lib/amd64/libverify.so
java    8262 root  mem    REG                8,3     43928   117391 /usr/lib64/librt-2.17.so
java    8262 root  mem    REG                8,3   1141456    93305 /usr/lib64/libm-2.17.so
java    8262 root  mem    REG                8,3  17068604 50673372 /usr/java/jdk1.8.0_181-amd64/jre/lib/amd64/server/libjvm.so
java    8262 root  mem    REG                8,3   2116736    93297 /usr/lib64/libc-2.17.so
java    8262 root  mem    REG                8,3     19344    93303 /usr/lib64/libdl-2.17.so
java    8262 root  mem    REG                8,3    104289 17064288 /usr/java/jdk1.8.0_181-amd64/lib/amd64/jli/libjli.so
java    8262 root  mem    REG                8,3    143352   117387 /usr/lib64/libpthread-2.17.so
java    8262 root  mem    REG                8,3    155064    93290 /usr/lib64/ld-2.17.so
java    8262 root  mem    REG                8,3      4096  1951892 /root/testfileio/out.txt
java    8262 root  mem    REG                8,3     32768 51320817 /tmp/hsperfdata_root/8262
java    8262 root    0u   CHR              136,0       0t0        3 /dev/pts/0
java    8262 root    1u   CHR              136,0       0t0        3 /dev/pts/0
java    8262 root    2u   CHR              136,0       0t0        3 /dev/pts/0
java    8262 root    3r   REG                8,3  66044248   210251 /usr/java/jdk1.8.0_181-amd64/jre/lib/rt.jar
java    8262 root    4u   REG                8,3      4096  1951892 /root/testfileio/out.txt
java    8262 root    5u  unix 0xffff88004357fc00       0t0    90276 socket
```
mem多了一项：`/tmp/hsperfdata_root/8262`，现在既可以使用MappedBYteBuffer（mmap）又可以用已有的文件描述符`/root/testfileio/out.txt`. 这时候
再来看文件内容：
```
[root@localhost testfileio]# cat out.txt
@@@looxxshibing
hello seanzhou
```
且文件大小涨到了4096：
```
[root@localhost testfileio]# ll
total 17520
...
...
-rw-r--r--. 1 root root    4096 Nov 15 00:27 out.txt
```
这是因为在代码里面有`MappedByteBuffer map = rafchannel.map(FileChannel.MapMode.READ_WRITE, 0, 4096);`
最后size那个参数被设置成了4096。

## 总结
Java梳到底是用C开发的一个程序，它的级别是Linux下的一个进程，代码段就是txt那一项，对一这个OS进程，他还有data和堆，这都是进程该有的。只不过在
堆里面，它可以根据我们给定的配置项-Xmx，在heap段，代码会跟系统申请分配一个1G的JVM的heap（堆里的堆）。如果用的是ByteBuffer.allocate是把
字节数组分配到了堆上（on heap）；如果用的是ByteBuffer.allocateDirect，则是分配到了Java进程的这个大的堆空间里面，不在JVM的堆里（off heap）。
学大数据和spark，一定要分清楚on/off heap。对于C程序来说，访问off heap的是直接访问，因为这段内存地址就是进程的线性地址空间；但是on heap
的内存是JVM的线性地址空间，并不是进程的，它需要一次翻译或者把它拷贝到off heap再使用。  

FileChannel会拿到一个MappedByteBuffer，通过`rafchannel.map`得到。他也是个字节数组，它位于Java进程的堆（大堆）的外面，会把用户进程的
线性地址空间和内核的pagecache映射起来（mmap），这个pagecache如果是脏的话，依然是要落地到磁盘文件（的某一个区域）里面去。这里在程序里只
需要写put方法，从程序进程的内存不产生系统调用就写到了pagecache，而dirty的pagecache刷入硬盘的话，会受内核的影响，pagecache也会丢数据。可见这个mmap减少了一次
系统调用，因为从java程序进程的堆里面写入pagecache的时候要走系统调用：channel.read()和channel.write()。但由于都是先写到了pagecache，
所以都有丢数据的可能。使用堆外的ByteBuffer性能比堆内的性能好，因为堆内的内容想写到pagecache，也要先写到堆外，再写到pagecache。
为什么还有堆内分配内存的方式？如果Object是我们能控制的，就可以分配一个堆外内存给他。我们不能控制的（具体以后再说）就放在堆内。性能来说，
on heap < off heap < mapped(仅限文件，file)。使用场景：Neyy （ByteBuffer on heap、off heap, off 更有优胜） Kafka log： mmap。
  
OS没有绝对的可靠性。设计pagecache的目的是减少硬件IO的调用，是想提速，优先使用内存。只要使用OS IO就有丢数据的风险，包括kafka。即便想要可靠性，
所以调成最慢的方式，**但是单点问题会让我们的性能损耗一点收益都没有。所以现在都做主从复制和主备HA.** 这就是为什么Kafka和ES都有副本的概念，
因为一个节点可能丢数据，但是很多节点做副本，就不太可能同时都丢数据了。通过socket得到这个副本的，socket他也是IO。综合考虑，多机器、多副本，
性能调到一个差不多的点上就可以了。在副本的时候又分为同步和异步。架构师之所以值钱，就是知识要一直堆，然后调整到一个合适的点上